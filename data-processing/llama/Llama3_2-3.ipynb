{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qtLWBACT0bVH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtLWBACT0bVH",
        "outputId": "5c7bb412-951a-4ca5-e994-723915f4122b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m154.2/156.5 MB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 3161a2bf073d6b22c4e2f33f951f3e5e3001462b2570e6df9cd57565bdec2984\n",
            "             Got        5fbd966532812d44f0e6c177eab22f980a269ab52666448baaabe396840a55a3\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes nltk unsloth pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R3G6Cfv-3spG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3G6Cfv-3spG",
        "outputId": "d5aff857-f052-4575-929f-95cd3c4a64d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-37RGNWm1lvm",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-37RGNWm1lvm",
        "outputId": "213d63bc-b3fa-445f-e823-bdd5b8611696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-9e761651108e>:19: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# LLM Code Translation Evaluation\n",
        "# Evaluating unsloth/Llama-3.2-3B-Instruct-bnb-4bit on code translation tasks\n",
        "\n",
        "# Step 1: Setup Environment\n",
        "!pip install -q transformers accelerate bitsandbytes nltk unsloth pandas matplotlib\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Download NLTK data for BLEU calculation\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 2: Load the Model and Tokenizer\n",
        "def load_model():\n",
        "    # Configure quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer with unsloth for faster inference\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model = model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Step 3: Define CodeBLEU functions\n",
        "# A simplified CodeBLEU implementation that focuses on basic aspects\n",
        "def calculate_bleu(reference, candidate):\n",
        "    \"\"\"Calculate BLEU score between reference and candidate.\"\"\"\n",
        "    # Tokenize reference and candidate\n",
        "    reference_tokens = nltk.word_tokenize(reference)\n",
        "    candidate_tokens = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score with smoothing\n",
        "    smooth = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate with different n-gram weights\n",
        "    weights_1 = (1.0, 0, 0, 0)  # BLEU-1\n",
        "    weights_2 = (0.5, 0.5, 0, 0)  # BLEU-2\n",
        "    weights_4 = (0.25, 0.25, 0.25, 0.25)  # BLEU-4\n",
        "\n",
        "    try:\n",
        "        bleu_1 = sentence_bleu([reference_tokens], candidate_tokens, weights=weights_1, smoothing_function=smooth)\n",
        "        bleu_2 = sentence_bleu([reference_tokens], candidate_tokens, weights=weights_2, smoothing_function=smooth)\n",
        "        bleu_4 = sentence_bleu([reference_tokens], candidate_tokens, weights=weights_4, smoothing_function=smooth)\n",
        "        return bleu_1, bleu_2, bleu_4\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BLEU: {e}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "def calculate_keyword_match(reference, candidate, language):\n",
        "    \"\"\"Calculate keyword matching score based on language keywords.\"\"\"\n",
        "    # Define keywords for each language\n",
        "    keywords = {\n",
        "        'python': ['def', 'class', 'if', 'elif', 'else', 'for', 'while', 'try', 'except',\n",
        "                  'import', 'from', 'return', 'with', 'as', 'True', 'False', 'None',\n",
        "                  'and', 'or', 'not', 'in', 'is', 'lambda'],\n",
        "        'java': ['public', 'private', 'protected', 'class', 'interface', 'extends', 'implements',\n",
        "                'void', 'int', 'boolean', 'String', 'if', 'else', 'for', 'while', 'try', 'catch',\n",
        "                'return', 'new', 'this', 'super', 'static', 'final', 'null', 'true', 'false'],\n",
        "        'cpp': ['int', 'float', 'double', 'char', 'bool', 'void', 'class', 'struct', 'enum',\n",
        "               'const', 'static', 'if', 'else', 'for', 'while', 'switch', 'case', 'break',\n",
        "               'return', 'try', 'catch', 'throw', 'new', 'delete', 'public', 'private', 'protected']\n",
        "    }\n",
        "\n",
        "    if language not in keywords:\n",
        "        print(f\"Warning: Keywords not defined for {language}\")\n",
        "        return 0\n",
        "\n",
        "    # Tokenize reference and candidate\n",
        "    ref_tokens = set(nltk.word_tokenize(reference))\n",
        "    cand_tokens = set(nltk.word_tokenize(candidate))\n",
        "\n",
        "    # Find language keywords in reference and candidate\n",
        "    ref_keywords = set(ref_tokens.intersection(keywords[language]))\n",
        "    cand_keywords = set(cand_tokens.intersection(keywords[language]))\n",
        "\n",
        "    # Calculate F1 score for keyword matching\n",
        "    if not ref_keywords:\n",
        "        return 0\n",
        "\n",
        "    precision = len(ref_keywords.intersection(cand_keywords)) / len(cand_keywords) if cand_keywords else 0\n",
        "    recall = len(ref_keywords.intersection(cand_keywords)) / len(ref_keywords) if ref_keywords else 0\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def calculate_codebleu(reference, candidate, target_language):\n",
        "    \"\"\"Calculate a simplified version of CodeBLEU.\"\"\"\n",
        "    # Calculate BLEU scores\n",
        "    bleu_1, bleu_2, bleu_4 = calculate_bleu(reference, candidate)\n",
        "\n",
        "    # Calculate keyword matching score\n",
        "    keyword_match = calculate_keyword_match(reference, candidate, target_language)\n",
        "\n",
        "    # Weighted sum (simplified CodeBLEU)\n",
        "    # 0.7 * BLEU + 0.3 * keyword_match\n",
        "    codebleu = 0.7 * bleu_4 + 0.3 * keyword_match\n",
        "\n",
        "    return {\n",
        "        'bleu_1': bleu_1,\n",
        "        'bleu_2': bleu_2,\n",
        "        'bleu_4': bleu_4,\n",
        "        'keyword_match': keyword_match,\n",
        "        'codebleu': codebleu\n",
        "    }\n",
        "\n",
        "\n",
        "def load_dataset_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Load a CSV with columns: problem_id, C++, Java, Python\n",
        "    and build a dict:\n",
        "      {\n",
        "        'cpp_to_java':   [ { 'source_code': ..., 'target_code': ... }, ... ],\n",
        "        'cpp_to_python': [ ... ],\n",
        "        'java_to_cpp':   [ ... ],\n",
        "        'java_to_python':[ ... ],\n",
        "        'python_to_cpp': [ ... ],\n",
        "        'python_to_java':[ ... ],\n",
        "      }\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    # map our lowercase keys to DataFrame column names\n",
        "    col_map = {'cpp': 'C++', 'java': 'Java', 'python': 'Python'}\n",
        "    dataset = {}\n",
        "\n",
        "    langs = ['cpp', 'java', 'python']\n",
        "    for src in langs:\n",
        "        for tgt in langs:\n",
        "            if src == tgt:\n",
        "                continue\n",
        "            key = f\"{src}_to_{tgt}\"\n",
        "            examples = []\n",
        "            for _, row in df.iterrows():\n",
        "                examples.append({\n",
        "                    'source_code': row[col_map[src]],\n",
        "                    'target_code': row[col_map[tgt]]\n",
        "                })\n",
        "            dataset[key] = examples[:10]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Step 5: Generate code translations using the LLM\n",
        "def generate_translation(model, tokenizer, source_code, source_lang, target_lang, max_new_tokens=1024):\n",
        "    \"\"\"Generate code translation using the LLM.\"\"\"\n",
        "    prompt = f\"\"\"Translate the following {source_lang} code to {target_lang}:\n",
        "\n",
        "```{source_lang}\n",
        "{source_code}\n",
        "```\n",
        "\n",
        "Translated {target_lang} code:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate translation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.1,  # Lower temperature for more deterministic output\n",
        "            num_beams=1,      # Greedy decoding\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the translated code part\n",
        "    translated_code = generated_text.split(\"Translated \" + target_lang + \" code:\")[1].strip()\n",
        "\n",
        "    # Remove any markdown code blocks if present\n",
        "    if translated_code.startswith(\"```\"):\n",
        "        translated_code = translated_code.split(\"```\")[1]\n",
        "        if target_lang.lower() in translated_code.lower().split(\"\\n\")[0]:\n",
        "            translated_code = \"\\n\".join(translated_code.split(\"\\n\")[1:])\n",
        "\n",
        "    # Remove ending code block if present\n",
        "    if \"```\" in translated_code:\n",
        "        translated_code = translated_code.split(\"```\")[0]\n",
        "\n",
        "    return translated_code.strip()\n",
        "\n",
        "# Step 6: Evaluate translations\n",
        "def evaluate_translations(model, tokenizer, dataset):\n",
        "    \"\"\"Evaluate the model on all translation directions.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Define the language pairs\n",
        "    language_pairs = [\n",
        "        ('java', 'python'),\n",
        "        ('java', 'cpp'),\n",
        "        ('python', 'java'),\n",
        "        ('python', 'cpp'),\n",
        "        ('cpp', 'java'),\n",
        "        ('cpp', 'python')\n",
        "    ]\n",
        "\n",
        "    # Run evaluations for each pair\n",
        "    for source_lang, target_lang in language_pairs:\n",
        "        pair_key = f\"{source_lang}_to_{target_lang}\"\n",
        "        print(f\"\\nEvaluating {pair_key}...\")\n",
        "\n",
        "        pair_results = []\n",
        "\n",
        "        # Skip if no examples for this pair\n",
        "        if pair_key not in dataset or not dataset[pair_key]:\n",
        "            print(f\"No examples found for {pair_key}\")\n",
        "            continue\n",
        "\n",
        "        # Process each example\n",
        "        for i, example in enumerate(tqdm(dataset[pair_key])):\n",
        "            source_code = example['source_code']\n",
        "            reference_code = example['target_code']\n",
        "\n",
        "            # Generate translation\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                translated_code = generate_translation(\n",
        "                    model, tokenizer, source_code, source_lang, target_lang\n",
        "                )\n",
        "                inference_time = time.time() - start_time\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = calculate_codebleu(reference_code, translated_code, target_lang)\n",
        "\n",
        "                # Store results\n",
        "                pair_results.append({\n",
        "                    'example_id': i,\n",
        "                    'source_code': source_code,\n",
        "                    'reference_code': reference_code,\n",
        "                    'translated_code': translated_code,\n",
        "                    'bleu_1': metrics['bleu_1'],\n",
        "                    'bleu_2': metrics['bleu_2'],\n",
        "                    'bleu_4': metrics['bleu_4'],\n",
        "                    'keyword_match': metrics['keyword_match'],\n",
        "                    'codebleu': metrics['codebleu'],\n",
        "                    'inference_time': inference_time\n",
        "                })\n",
        "\n",
        "                print(f\"Example {i} - CodeBLEU: {metrics['codebleu']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing example {i}: {e}\")\n",
        "\n",
        "        # Store results for this pair\n",
        "        results[pair_key] = pair_results\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SdR2fxB264Nv",
      "metadata": {
        "id": "SdR2fxB264Nv"
      },
      "outputs": [],
      "source": [
        "# Step 7: Analyze results\n",
        "def analyze_results(results):\n",
        "    \"\"\"Analyze and visualize evaluation results.\"\"\"\n",
        "    # Calculate average metrics for each translation direction\n",
        "    summary = {}\n",
        "\n",
        "    for pair, pair_results in results.items():\n",
        "        if not pair_results:\n",
        "            continue\n",
        "\n",
        "        avg_bleu_1 = np.mean([r['bleu_1'] for r in pair_results])\n",
        "        avg_bleu_2 = np.mean([r['bleu_2'] for r in pair_results])\n",
        "        avg_bleu_4 = np.mean([r['bleu_4'] for r in pair_results])\n",
        "        avg_keyword = np.mean([r['keyword_match'] for r in pair_results])\n",
        "        avg_codebleu = np.mean([r['codebleu'] for r in pair_results])\n",
        "        avg_time = np.mean([r['inference_time'] for r in pair_results])\n",
        "\n",
        "        summary[pair] = {\n",
        "            'avg_bleu_1': avg_bleu_1,\n",
        "            'avg_bleu_2': avg_bleu_2,\n",
        "            'avg_bleu_4': avg_bleu_4,\n",
        "            'avg_keyword_match': avg_keyword,\n",
        "            'avg_codebleu': avg_codebleu,\n",
        "            'avg_inference_time': avg_time,\n",
        "            'num_examples': len(pair_results)\n",
        "        }\n",
        "\n",
        "    # Create a summary dataframe\n",
        "    df_summary = pd.DataFrame({\n",
        "        'Translation': list(summary.keys()),\n",
        "        'Avg BLEU-1': [summary[k]['avg_bleu_1'] for k in summary],\n",
        "        'Avg BLEU-2': [summary[k]['avg_bleu_2'] for k in summary],\n",
        "        'Avg BLEU-4': [summary[k]['avg_bleu_4'] for k in summary],\n",
        "        'Avg Keyword Match': [summary[k]['avg_keyword_match'] for k in summary],\n",
        "        'Avg CodeBLEU': [summary[k]['avg_codebleu'] for k in summary],\n",
        "        'Avg Inference Time (s)': [summary[k]['avg_inference_time'] for k in summary],\n",
        "        'Examples': [summary[k]['num_examples'] for k in summary]\n",
        "    })\n",
        "\n",
        "    print(\"\\n===== EVALUATION SUMMARY =====\")\n",
        "    print(df_summary)\n",
        "\n",
        "    # Visualize results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    bar_width = 0.15\n",
        "    r1 = np.arange(len(summary))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "    r4 = [x + bar_width for x in r3]\n",
        "    r5 = [x + bar_width for x in r4]\n",
        "\n",
        "    plt.bar(r1, df_summary['Avg BLEU-1'], width=bar_width, label='BLEU-1')\n",
        "    plt.bar(r2, df_summary['Avg BLEU-2'], width=bar_width, label='BLEU-2')\n",
        "    plt.bar(r3, df_summary['Avg BLEU-4'], width=bar_width, label='BLEU-4')\n",
        "    plt.bar(r4, df_summary['Avg Keyword Match'], width=bar_width, label='Keyword Match')\n",
        "    plt.bar(r5, df_summary['Avg CodeBLEU'], width=bar_width, label='CodeBLEU')\n",
        "\n",
        "    plt.xlabel('Translation Direction', fontweight='bold')\n",
        "    plt.ylabel('Score', fontweight='bold')\n",
        "    plt.xticks([r + bar_width*2 for r in range(len(summary))], df_summary['Translation'])\n",
        "    plt.title('Code Translation Performance Metrics')\n",
        "    plt.legend()\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('code_translation_metrics.png')\n",
        "    plt.show()\n",
        "\n",
        "    return df_summary\n",
        "\n",
        "# Step 8: Save results\n",
        "def save_results(results, summary, output_dir='results'):\n",
        "    \"\"\"Save evaluation results and summary.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save detailed results\n",
        "    with open(f'{output_dir}/detailed_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Save summary\n",
        "    summary.to_csv(f'{output_dir}/summary.csv', index=False)\n",
        "\n",
        "    print(f\"\\nResults saved to {output_dir}/\")\n",
        "\n",
        "# Step 9: Main evaluation function\n",
        "def main():\n",
        "    print(\"Starting LLM Code Translation Evaluation...\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    print(\"\\nLoading model and tokenizer...\")\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    # Prepare dataset\n",
        "    print(\"\\nPreparing dataset...\")\n",
        "    dataset = load_dataset_from_file('/content/all_languages_aligned.csv')\n",
        "\n",
        "    # Alternatively, load your own dataset\n",
        "    # dataset = load_dataset_from_file('your_dataset.json')\n",
        "\n",
        "    # Run evaluations\n",
        "    print(\"\\nRunning evaluations...\")\n",
        "    results = evaluate_translations(model, tokenizer, dataset)\n",
        "\n",
        "    # Analyze results\n",
        "    print(\"\\nAnalyzing results...\")\n",
        "    summary = analyze_results(results)\n",
        "\n",
        "    # Save results\n",
        "    save_results(results, summary)\n",
        "\n",
        "    print(\"\\nEvaluation completed!\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}